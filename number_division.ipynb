{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(1234) # 난수 발생 패턴 고정\n",
    "def randomize(): np.random.seed(time.time()) # 난수 발생 패턴 재설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_MEAN = 0 # 정수분포 난숫값의 평균\n",
    "RND_STD = 0.0030 # 정수분포 난숫값의 표준편차\n",
    "\n",
    "LEARNING_RATE = 0.001 # 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steel_dataset():\n",
    "    \n",
    "    global input_cnt, output_cnt, data, every_data, data_20, data_8, show_data\n",
    "    \n",
    "    input_cnt = 401\n",
    "    output_cnt = 5\n",
    "\n",
    "    os.chdir('./20_data')\n",
    "    file_names_20=os.listdir() # 20 x 20 학습데이터가 들어있는 폴더 안에 있는 파일을 모두 읽어옴\n",
    "    os.chdir('../8_data')\n",
    "    file_names_8=os.listdir() # 8 x 8 학습데이터가 들어있는 폴더 안에 있는 파일을 모두 읽어옴\n",
    "    \n",
    "    data_20=np.zeros([len(file_names_20),input_cnt+output_cnt])\n",
    "    data_8=np.zeros([len(file_names_8),input_cnt+output_cnt])\n",
    "    # data = np.zeros([len(file_names_20)+len(file_names_8), input_cnt+output_cnt]) #학습 데이터 n개 x 406([픽셀값 400] + [20,8 사이즈 판단 1] + [1~5정답값 5개]) 배열\n",
    "    show_data=np.zeros([len(file_names_20)+len(file_names_20),20,20]) #학습 데이터 입력값 n x 400 배열 (그냥 이미지 픽셀)\n",
    "\n",
    "    data_idx=0;\n",
    "\n",
    "    # Every_Data 안에 있는 파일의 픽셀값과 정답을 data배열에 넘김\n",
    "    for filename in file_names_20:\n",
    "        \n",
    "        # 파일을 이미지로 받아오고 흑백 이미지의 픽셀 배열로 변환\n",
    "        image=Image.open('../20_data/'+filename)\n",
    "        image=image.convert('L')\n",
    "        pix=np.array(image)\n",
    "        pix=pix[1:,:]\n",
    "        # show_data에 2차원 배열로 이미지 저장\n",
    "        show_data[data_idx]=pix\n",
    "        \n",
    "        # data 배열의 :400 까지는 1차원 픽셀값 저장 / 400: 부터는 정답값 저장 \n",
    "        pix=np.concatenate(pix).tolist()\n",
    "        data_20[data_idx,:input_cnt-1] = pix\n",
    "        answer=os.path.splitext(filename)[0][0]\n",
    "        data_20[data_idx,input_cnt-1]=0 # 400번째 인덱스에 20 x 20 이미지라는 것을 표시\n",
    "        data_20[data_idx,input_cnt-1+int(answer)]=1 # 401~405까지의 인덱스에는 1~5까지 값 중에 무엇인지 표시\n",
    "        data_idx+=1\n",
    "        \n",
    "    data_idx=0\n",
    "    \n",
    "    for filename in file_names_8:\n",
    "        \n",
    "        # 파일을 이미지로 받아오고 흑백 이미지의 픽셀 배열로 변환\n",
    "        image=Image.open('../8_data/'+filename)\n",
    "        image=image.convert('L')\n",
    "        pix=np.array(image)\n",
    "        pix=pix[1:,:]\n",
    "        \n",
    "        # show_data에 2차원 배열로 이미지 저장\n",
    "        show_data[data_idx]=pix\n",
    "        \n",
    "        # data 배열의 :400 까지는 1차원 픽셀값 저장 / 400: 부터는 정답값 저장 \n",
    "        pix=np.concatenate(pix).tolist()\n",
    "        data_8[data_idx,:input_cnt-1] = pix\n",
    "        answer=os.path.splitext(filename)[0][0]\n",
    "        data_8[data_idx,input_cnt-1]=1 # 400번째 인덱스에 8 x 8 이미지라는 것을 표시\n",
    "        data_8[data_idx,input_cnt-1+int(answer)]=1 # 401~405까지의 인덱스에는 1~5까지 값 중에 무엇인지 표시\n",
    "        data_idx+=1\n",
    "        \n",
    "    every_data=np.concatenate((data_20,data_8),axis=0)\n",
    "\n",
    "    data=_data\n",
    "        \n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    global weight, bias, input_cnt, output_cnt\n",
    "    weight = np.random.normal(RND_MEAN, RND_STD,[input_cnt, output_cnt]) #가중치 행렬을 정규분포를 갖는 10 X 1 의 난숫값으로 초기화\n",
    "    bias = np.zeros([output_cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(mb_size):\n",
    "    global data, shuffle_map, test_begin_idx\n",
    "    shuffle_map = np.arange(data.shape[0]) #data.shape[0]=데이터 갯수 1~데이터갯수만큼의 배열 생성\n",
    "    np.random.shuffle(shuffle_map) # 무작위로 순서 섞기\n",
    "    step_count = int(data.shape[0] * 0.8) // mb_size # 학습용 데이터와 테스트용 데이터 경계 나눔\n",
    "    test_begin_idx = step_count * mb_size # 테스트 데이터 시작 인덱스\n",
    "    return step_count\n",
    "\n",
    "def get_test_data():\n",
    "    global data, shuffle_map, test_begin_idx, output_cnt\n",
    "    test_data = data[shuffle_map[test_begin_idx:]] #test_data 테스트 시작 인덱스부터의 data 배열\n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:] #test_data 배열의 입력값과 그에 따른 목표값 각각 반환\n",
    "\n",
    "def get_train_data(mb_size, nth):\n",
    "    global data, shuffle_map, test_begin_idx, output_cnt\n",
    "    if nth == 0:\n",
    "        np.random.shuffle(shuffle_map[:test_begin_idx]) # 한번의 학습 데이터셋이 학습될 때마다 학습 데이터를 바꿔줌\n",
    "    train_data = data[shuffle_map[mb_size*nth:mb_size*(nth+1)]] # 10개의 데이터마다 한번씩 학습시킴\n",
    "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:] #train_data 배열의 입력값과 그에 따른 목표값 각각 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_elem = np.max(x, axis=1)\n",
    "    diff = (x.transpose() - max_elem).transpose()\n",
    "    exp = np.exp(diff)\n",
    "    sum_exp = np.sum(exp, axis=1)\n",
    "    probs = (exp.transpose() / sum_exp).transpose()\n",
    "    return probs\n",
    "\n",
    "def softmax_derv(x, y):\n",
    "    mb_size, nom_size = x.shape\n",
    "    derv = np.ndarray([mb_size, nom_size, nom_size])\n",
    "    for n in range(mb_size):\n",
    "        for i in range(nom_size):\n",
    "            for j in range(nom_size):\n",
    "                derv[n, i, j] = -y[n,i] * y[n,j]\n",
    "            derv[n, i, i] += y[n,i]\n",
    "    return derv\n",
    "\n",
    "def softmax_cross_entropy_with_logits(labels, logits):\n",
    "    probs = softmax(logits)\n",
    "    return -np.sum(labels * np.log(probs+1.0e-10), axis=1)\n",
    "\n",
    "def softmax_cross_entropy_with_logits_derv(labels, logits):\n",
    "    return softmax(logits) - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_postproc(output, y):\n",
    "    entropy = softmax_cross_entropy_with_logits(y, output)\n",
    "    loss = np.mean(entropy) \n",
    "    return loss, [y, output, entropy]\n",
    "\n",
    "def backprop_postproc(G_loss, aux):\n",
    "    y, output, entropy = aux\n",
    "    \n",
    "    g_loss_entropy = 1.0 / np.prod(entropy.shape)\n",
    "    g_entropy_output = softmax_cross_entropy_with_logits_derv(y, output)\n",
    "    \n",
    "    G_entropy = g_loss_entropy * G_loss\n",
    "    G_output = g_entropy_output * G_entropy\n",
    "    \n",
    "    return G_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, y):\n",
    "    estimate = np.argmax(output, axis=1)\n",
    "    answer = np.argmax(y, axis=1)\n",
    "    correct = np.equal(estimate, answer)\n",
    "    \n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet(x):\n",
    "    global weight, bias\n",
    "    output = np.matmul(x, weight) + bias\n",
    "    return output, x\n",
    "\n",
    "def backprop_neuralnet(G_output, x):\n",
    "    global weight, bias\n",
    "    g_output_w = x.transpose()\n",
    "    \n",
    "    G_w = np.matmul(g_output_w, G_output)\n",
    "    G_b = np.sum(G_output, axis=0)\n",
    "\n",
    "    weight -= LEARNING_RATE * G_w\n",
    "    bias -= LEARNING_RATE * G_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(x, y):\n",
    "    output, aux_nn = forward_neuralnet(x) # 순전파 행렬 계산\n",
    "    loss, aux_pp = forward_postproc(output, y) # 오차 계산 (평균제곱오차, 오차)\n",
    "    accuracy = eval_accuracy(output, y) # \n",
    "    \n",
    "    G_loss = 1.0\n",
    "    G_output = backprop_postproc(G_loss, aux_pp) #제곱오차값을 통해 \n",
    "    backprop_neuralnet(G_output, aux_nn) # 역전파 행렬 계산\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def run_test(x, y):\n",
    "    output, _ = forward_neuralnet(x)\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_count, mb_size, report):\n",
    "    step_count = arrange_data(mb_size) # 무작위로 데이터를 섞고 데이터 갯수를 학습 횟수로 나눈 값 반환\n",
    "    test_x, test_y = get_test_data() #data의 나머지 20프로를 test_data의 입력값, 출력값으로 추출\n",
    "     \n",
    "    \n",
    "    for epoch in range(epoch_count): # 학습 횟수만큼씩 돌아가면서\n",
    "        losses, accs = [], [] # 각 epoch의 loss, accs 초기화\n",
    "        \n",
    "        for n in range(step_count): # 0~step_count까지\n",
    "            train_x, train_y = get_train_data(mb_size, n) #0*10 ~ step_cout*10 까지 10개씩 학습시킴\n",
    "            loss, acc = run_train(train_x, train_y) \n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            \n",
    "            \n",
    "        if report > 0 and (epoch+1) % report == 0: # 하나의 데이터셋에 대해 학습이 끝나면\n",
    "            acc = run_test(test_x, test_y) # get_test_data를 통해 얻는 test값들을 통해 테스트\n",
    "            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'. \\\n",
    "                  format(epoch+1, np.mean(losses), np.mean(accs), acc)) # 횟수, 오차, 정확도 출력\n",
    "            \n",
    "    final_acc = run_test(test_x, test_y)\n",
    "    print('\\nFinal Test: final accuracy = {:5.3f}'.format(final_acc)) # 마지막 학습에 대해 횟수, 오차, 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steel_exec(_data,epoch_count=10, mb_size=10, report=1):\n",
    "    load_steel_dataset(_data)\n",
    "    init_model()\n",
    "    train_and_test(epoch_count, mb_size, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=17.378, accuracy=0.242/0.167\n",
      "Epoch 2: loss=17.712, accuracy=0.231/0.236\n",
      "Epoch 3: loss=17.889, accuracy=0.223/0.250\n",
      "Epoch 4: loss=18.198, accuracy=0.208/0.236\n",
      "Epoch 5: loss=18.598, accuracy=0.192/0.167\n",
      "Epoch 6: loss=17.978, accuracy=0.219/0.167\n",
      "Epoch 7: loss=17.889, accuracy=0.223/0.250\n",
      "Epoch 8: loss=17.624, accuracy=0.235/0.236\n",
      "Epoch 9: loss=17.978, accuracy=0.219/0.278\n",
      "Epoch 10: loss=18.421, accuracy=0.200/0.250\n",
      "\n",
      "Final Test: final accuracy = 0.250\n"
     ]
    }
   ],
   "source": [
    "os.chdir('c:/Users/taebe/Desktop/Joe/Embeded/2022_2/인공지능/To Student 2022/Code/chap03')\n",
    "steel_exec(data_20,10,10,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "733f4186b6a0f157d658c039fce7b9e10518bd63d06106775571285885657c18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
