# 2022_2_AI
---
## 2022년 2학기 인공지능 과목

### 1. 순전파

- **행렬**로 순전파 계산 표현하기
    - $X = W*I$
        - $W$: 가중치 행렬, $I$: 입력값 행렬, $X$: 다음 계층의 입력 행렬
        - 행렬을 통해 복잡한 계산을 간단한 형태로 표현할 수 있으며 행렬 기반의 연산으로 빠르게 컴퓨터가 처리할 수 있다.
    - $O = sigmoid(X)$
        - $O:$ 노드의 출력값
        - 입력값과 가중치의 곱의 합에 시그모이드를 취한 것이 최종 노드의 출력값
- 순전파 계산을 통해 (최종 노드)출력 노드의 출력값을 구할 수 있고, 그렇다면 목표값에서 출력값을 뺀 오차값을 구할 수 있음. 출력의 오차값을 줄이기 위해서는 입력값은 정해져 있고, 목표값도 정해져 있으므로 Weight(가중치)값을 조정해야 함


### 2.오차의 역전파

- 오차: 예측값과 실제값의 차이 (목표값 - 출력값)
- 오차값을 보고 실제값에 가까워지도록 가중치값을 업데이트 해야 한다. 여기서 고려해야 할 점은
    - 오차를 **어떤 비율**로 전파할지
        - **노드 개수**만큼 나누는 방법
        - **가중치**에 비례하여 나누는 방법
    - **은닉층의 오차**를 어떻게 구할지
        - 출력값은 마지막 출력 노드만 가지고 있으므로 은닉층의 오차값을 구할 필요가 있다.
- 신경망의 순전파에는 N번째 계층의 노드에 N-1번째 계층의 `**모든 노드**`가 관여하기 때문에 모든 노드가 결과값에 지분을 가지고 있다.

#### 2-1.**오차의 행렬곱**으로 **은닉층의 오차** 찾기

- $Error_{hidden} = W^T_{hidden-output} *Error_{output}$

 $$error_{hidden} = \left[
\begin{matrix}
    w_{11}/w_{11}+w_{21} & w_{12}/w_{12}+w_{22} \\
    w_{21}/w_{11}+w_{21} & w_{22}/w_{12}+w_{22} \\
\end{matrix}
\right]*\left[
\begin{matrix}
    e1 \\
    e2 \\
\end{matrix}
\right]$$

#### 2-2**휴리스틱**하게 오차 계산하기

</aside>

$$error_{hidden} =  \left[
\begin{matrix}
    w_{11} & w_{12} \\
    w_{21} & w_{22}\\
\end{matrix}
\right]*\left[
\begin{matrix}
    e1 \\
    e2 \\
\end{matrix}
\right]$$

- 핵심값은 출력 오차인 $e_n$과 가중치 $w_{ij}$이며 분모 부분을 잃는다고 해도 오차의 비율은 똑같이 전파되기 때문에 분모 부분을 생략하고 표현할 수 있음
- 순전파 계산의 가중치 행렬과 **‘전치’** 관계
    - 전치: 행렬의 행과 열이 뒤바뀐 것
 
### 3. 가중치 업데이트

#### 3-1. 신경망 학습 과정

초기 입력값과 초기 가중치를 이용해 **순전파**를 시켜서 최종 출력을 만들어낸다. 최종 오차값을 **역전파**시키고 ‘가중치-오차’ 간의 관계를 정의한 오차함수를 미분하여 가중치를 수정할 올바른 방향과, 정도를 설정하여 **가중치를 업데이트**해야 함

#### 3-2. 가중치 업데이트

- 오차와 가중치의 관계에서, 오차를 최소화시키는 가중치를 찾아야 함
- 오차함수를 통해 가중치 값 업데이트 하기
    - 수학적으로 가중치와 오차의 관계를 표현하는 오차함수 정의
    - 가중치로 편미분 (기울기를 찾기 위해)
    - 편미분한 값을 바탕으로 경사하강법과 학습률을 적용하여 오차가 최소값이 되도록 가중치 값을 업데이트

#### 3-3. 가중치 업데이트 - 경사하강법

- 말 그대로 경사를 따라 하강하는 ‘길’을 찾는 것
- **휴리스틱 방식**으로 최적의 해는 아니지만 빠르게 결론을 도출해낼 수 있음
    - **많은 매개변수**를 가진 함수라면 대수학을 통해 한번에 최저점을 구하기 어려움. 사실상 불가능
- 최저점을 찾는다고 할 때, 미분과 경사하강법을 통해 최저점을 찾을 수 있다. 임의의 위치에서 미분을 하여 기울기를 구했을 때 음수라면 오른쪽으로 이동해야 하고, 양수라면 왼쪽으로 이동해야 한다. 그리고 미분한 기울기 값이 크다는 것은 가파르고 최저점과 멀어 더욱 많이 이동해야 한다는 것이고 작다는 것은 완만하여 최저점에 가깝기 떄문에 적게 이동해도 된다는 것
- 하지만 이러한 기울기를 통한 경사하강법에서는 잘못된 최저점에 도달할 수 있다. Local Minimum에서 결론이 도출되는 경우가 있을 수 있다. 시작 위치를 다르게 하는 것과 간격을 어떻게 하느냐에 따라 결과가 달라질 수 있다.

#### 3-4. 가중치 업데이트 - 오차값 정의

- 목표값 - 실제값
    - 말그대로 오차를 그대로 적용해보는 것
    - 오차가 양수와 음수가 나와서 계산 와중에 상쇄될 수 있기 때문에 정확하지 않을 수 있음
- | 목표값 - 실제값 |
    - 절대치를 취하는 것
    - 상쇄되는 게 없고 크기만 남음
    - 불연속점이 생긴다는 단점이 있음 (미분이 불가능)
- | 목표값 - 실제값 | $^2$
    - 목표값- 실제값의 제곱 식으로 구해야 함 (제곱오차 방식)
    - 모두 양수이기에 상쇄되지도 않고 연속적이므로 값의 흐름이 아주 자연스러움

#### 3-5. 가중치 업데이트 - 오차함수 미분

> 입력계층 $i$, 은닉계층 $j$, 출력계층 $k$를 가지는 3layer $*$ 2node 신경망이 있을 때 은닉계층 $j$에서 출력계층 $k$를 잇는 $w_{jk}$ 와 출력값 $o_k$를 통해 오차함수를 만들고 미분하여 가중치 $w_{jk}$를 업데이트 하려고 함
> 
- 오차의 정의
    
    $E = \sum_{n}(t_n - o_n)^2$
    
- 오차를 가중치에 대해 미분을 해야 기울기를 알 수 있고, 기울기를 알아야 방향을 알 수 있음 미분을 적용해보면
    
    ${d\over dw_{jk}}E= {d\over dw_{jk}}\sum_{n}(t_n-o_n)^2$
    
- 노드 $k$에 대한 값만 확인하기 위해서는 필요없는 값들을 제거할 수 있음
    
    = ${d\over dw_{jk}}(t_k-o_k)^2$
    
- 이것을 연쇄법칙으로 미분하면
    
    = ${d(t_k-o_k)^2\over do_k}*$$do_k\over dw_{jk}$
    
    = ${-2(t_k-o_k)} * {{do_k}\over dw_{jk}}$ 
    
- 이렇게 되면 $t_k-o_k$값이 컴퓨터에서 계산할 수 있는 상수값이 됨
그리고 $o_k$는 $k$노드의 시그모이드가 적용된 출력값이므로
    
    $o_k=sigmoid(\sum_jw_{jk}*o_j)$
    
- 따라서 $o_k$ 값을 $sigmoid(\sum_jw_{jk}*o_j)$ 으로 바꿔서 원래의 식에 적용하면
    
    = $-2(t_k-o_k)*{d\over dw_{jk}}sigmoid(\sum_jw_{jk}*o_j)$
    
- 시그모이드 미분법
    
    ${d\over dx}sigmoid(x) = sigmoid(x)(1-sigmoid(x))$
    
- 시그모이드 미분법을 원래의 식에 적용하면
    
    $-2(t_k-o_k)*sigmoid(\sum_jw_{jk}*o_j)(1-sigmoid(\sum_jw_{jk}*o_j)){d\over dw_{jk}}\sum_jw_{jk}*o_j$
    
- ${d\over dw_{jk}}\sum_jw_{jk}*o_j$ 는 $o_j$가 됨
    
    $-2(t_k-o_k)*sigmoid(\sum_jw_{jk}*o_j)(1-sigmoid(\sum_jw_{jk}*o_j))*o_j$
    
- 기울기 방향만 알면 되므로 2는 제거할 수 있음
    
    $-(t_k-o_k)*sigmoid(\sum_jw_{jk}*o_j)(1-sigmoid(\sum_jw_{jk}*o_j))*o_j$
    
- 이렇게 해서 최종적으로 컴퓨터가 계산할 수 있는 형태의 오차함수 미분값을 구할 수 있음

#### 3-6. 가중치 업데이트 - 미분값에 영향을 미치는 요소

- $t_k-o_k = E_k$ : ‘목표값 - 출력값’으로 해당 노드의 오차값
- $sigmoid(\sum_jw_{jk}*o_j)=O_k$ : 출력노드 $k$의 출력값
- $o_j$ : 은닉 계층 $j$의 출력값
- 따라서 가중치 업데이트 값은
    
    $\delta w_{jk}=\alpha*E_kO_k(1-O_k)*O_j$

#### 3-7. 가중치 업데이트 - 학습률

- 미분한 결과에 따라서 가중치를 업데이트 할 수 있음
- 학습률을 오버슈팅을 방지하기 위해 변화의 강도를 조정하는 역할을 함
- 학습률을 적용하지 않았을 때
    - $new~w_{jk} = old ~w_{jk} - {dE\over dw_{jk}}$
- 학습률 $\alpha$를 적용했을 때
    - $new~w_{jk} = old ~w_{jk} - \alpha*{dE\over dw_{jk}}$
